{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you've already loaded your DataFrame `alignment_df` from the CSV\n",
    "directory_path = '/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/data/raw-txt'\n",
    "\n",
    "alignment_df = pd.read_csv('/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/data/combined_csv.csv')\n",
    "def ensure_txt_extension(filename):\n",
    "    if pd.isnull(filename):\n",
    "        return None  # Handles NaN values gracefully\n",
    "    if not filename.endswith('.txt'):\n",
    "        return f\"{filename}.txt\"\n",
    "    return filename\n",
    "alignment_df['filename'] = alignment_df['filename'].apply(ensure_txt_extension)\n",
    "\n",
    "# Function to read text from a .txt file\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "documents_list = []\n",
    "ranks = []\n",
    "uni_names = []\n",
    "\n",
    "for index, row in alignment_df.iterrows():\n",
    "    # Check if the filename is not NaN (not a float)\n",
    "    if not pd.isna(row['filename']) and isinstance(row['filename'], str):\n",
    "        file_path = os.path.join(directory_path, row['filename'])\n",
    "        if os.path.exists(file_path):\n",
    "            text = read_txt(file_path)\n",
    "            documents_list.append(text)\n",
    "            ranks.append(row['qs_ranking'])\n",
    "            uni_names.append(row['institution'])\n",
    "        else:\n",
    "            print(f\"File does not exist: {file_path}\")\n",
    "    else:\n",
    "        # Handle the case where filename is NaN or not a string\n",
    "        print(f\"Invalid or missing filename at index {index}\")\n",
    "\n",
    "# Convert lists to DataFrame\n",
    "documents_df = pd.DataFrame({\n",
    "    'document': documents_list,\n",
    "    'rank': ranks,\n",
    "    'institution': uni_names\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>rank</th>\n",
       "      <th>institution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FINAL : APPROVED DECEMBER 8 , 2022 \\n \\n20...</td>\n",
       "      <td>102</td>\n",
       "      <td>University of Wisconsin-Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLAN\\n2020 • 2025DEVELOPMENT\\nPLAN\\n2020 • 202...</td>\n",
       "      <td>103</td>\n",
       "      <td>The Pontifical Catholic University of Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GO FURTHER2021–2025\\nTransforming lives  \\nthr...</td>\n",
       "      <td>104</td>\n",
       "      <td>The University of Sheffield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n UFV 2018/641  \\nUppsala University: \\nMis...</td>\n",
       "      <td>105</td>\n",
       "      <td>Uppsala University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>university of copenhagen\\nSTRATEGY 2030\\nCreat...</td>\n",
       "      <td>107</td>\n",
       "      <td>University of Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>BSU BSU ––an an \\nexperienced experienced \\nan...</td>\n",
       "      <td>387</td>\n",
       "      <td>Belarusian State University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>July 2023 - Case Number : 2019FR470010  \\n  \\n...</td>\n",
       "      <td>392</td>\n",
       "      <td>Institut National des Sciences AppliquÃ©es de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Prospective Student (http://sites.scut.edu.cn/...</td>\n",
       "      <td>392</td>\n",
       "      <td>South China University of Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>THE DEVELOPMENT PLAN OF JUSTUS LIEBIG UNIVERSI...</td>\n",
       "      <td>396</td>\n",
       "      <td>Justus-Liebig-University Giessen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>\\n                                          \\...</td>\n",
       "      <td>399</td>\n",
       "      <td>HSE University</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              document  rank  \\\n",
       "0        FINAL : APPROVED DECEMBER 8 , 2022 \\n \\n20...   102   \n",
       "1    PLAN\\n2020 • 2025DEVELOPMENT\\nPLAN\\n2020 • 202...   103   \n",
       "2    GO FURTHER2021–2025\\nTransforming lives  \\nthr...   104   \n",
       "3      \\n UFV 2018/641  \\nUppsala University: \\nMis...   105   \n",
       "4    university of copenhagen\\nSTRATEGY 2030\\nCreat...   107   \n",
       "..                                                 ...   ...   \n",
       "248  BSU BSU ––an an \\nexperienced experienced \\nan...   387   \n",
       "249  July 2023 - Case Number : 2019FR470010  \\n  \\n...   392   \n",
       "250  Prospective Student (http://sites.scut.edu.cn/...   392   \n",
       "251  THE DEVELOPMENT PLAN OF JUSTUS LIEBIG UNIVERSI...   396   \n",
       "252   \\n                                          \\...   399   \n",
       "\n",
       "                                           institution  \n",
       "0                     University of Wisconsin-Madison   \n",
       "1          The Pontifical Catholic University of Chile  \n",
       "2                          The University of Sheffield  \n",
       "3                                   Uppsala University  \n",
       "4                             University of Copenhagen  \n",
       "..                                                 ...  \n",
       "248                        Belarusian State University  \n",
       "249  Institut National des Sciences AppliquÃ©es de ...  \n",
       "250               South China University of Technology  \n",
       "251                   Justus-Liebig-University Giessen  \n",
       "252                                     HSE University  \n",
       "\n",
       "[253 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nachiketh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nachiketh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nachiketh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nachiketh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m phraser \u001b[39m=\u001b[39m detect_phrases(tokenized_docs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Now preprocess documents including the detected phrases\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m processed_docs_with_phrases \u001b[39m=\u001b[39m [preprocess_with_phrases(text, phraser) \u001b[39mfor\u001b[39;49;00m text \u001b[39min\u001b[39;49;00m documents_df[\u001b[39m'\u001b[39;49m\u001b[39mdocument\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Continue with dictionary and bow_corpus creation using the processed_docs_with_phrases\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m dictionary \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mcorpora\u001b[39m.\u001b[39mDictionary(processed_docs_with_phrases)\n",
      "\u001b[1;32m/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m phraser \u001b[39m=\u001b[39m detect_phrases(tokenized_docs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Now preprocess documents including the detected phrases\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m processed_docs_with_phrases \u001b[39m=\u001b[39m [preprocess_with_phrases(text, phraser) \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m documents_df[\u001b[39m'\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Continue with dictionary and bow_corpus creation using the processed_docs_with_phrases\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m dictionary \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mcorpora\u001b[39m.\u001b[39mDictionary(processed_docs_with_phrases)\n",
      "\u001b[1;32m/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m    words \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39msplit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m    words \u001b[39m=\u001b[39m phraser[words] \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m    words \u001b[39m=\u001b[39m remove_non_english_words(words)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Apply phrase model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m    \u001b[39m# filtered_text = [word for word in words if word not in stop_words]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m    filtered_text \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(word) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m  \u001b[39mand\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n",
      "\u001b[1;32m/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_non_english_words\u001b[39m(words):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Filter out words that are not detected as English\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     filtered_words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m words \u001b[39mif\u001b[39;49;00m detect(word) \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m filtered_words\n",
      "\u001b[1;32m/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_non_english_words\u001b[39m(words):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Filter out words that are not detected as English\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     filtered_words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m detect(word) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/main.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m filtered_words\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector_factory.py:130\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    128\u001b[0m detector \u001b[39m=\u001b[39m _factory\u001b[39m.\u001b[39mcreate()\n\u001b[1;32m    129\u001b[0m detector\u001b[39m.\u001b[39mappend(text)\n\u001b[0;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m detector\u001b[39m.\u001b[39;49mdetect()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector.py:136\u001b[0m, in \u001b[0;36mDetector.detect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    133\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Detect language of the target text and return the language name\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    which has the highest probability.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_probabilities()\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m probabilities:\n\u001b[1;32m    138\u001b[0m         \u001b[39mreturn\u001b[39;00m probabilities[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlang\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector.py:143\u001b[0m, in \u001b[0;36mDetector.get_probabilities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_probabilities\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlangprob \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_detect_block()\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_probability(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlangprob)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector.py:161\u001b[0m, in \u001b[0;36mDetector._detect_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_lang_prob(prob, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(ngrams), alpha)\n\u001b[1;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    163\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_prob(prob) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONV_THRESHOLD \u001b[39mor\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mITERATION_LIMIT:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector.py:212\u001b[0m, in \u001b[0;36mDetector._update_lang_prob\u001b[0;34m(self, prob, word, alpha)\u001b[0m\n\u001b[1;32m    210\u001b[0m weight \u001b[39m=\u001b[39m alpha \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBASE_FREQ\n\u001b[1;32m    211\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m xrange(\u001b[39mlen\u001b[39m(prob)):\n\u001b[0;32m--> 212\u001b[0m     prob[i] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m weight \u001b[39m+\u001b[39m lang_prob_map[i]\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "from langdetect import detect\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Function to detect phrases in the documents\n",
    "def detect_phrases(docs):\n",
    "    phrases = Phrases(docs, min_count=5, threshold=12) # Play with these parameters based on your corpus\n",
    "    phraser = Phraser(phrases)\n",
    "    return phraser\n",
    "\n",
    "def remove_non_english_words(words):\n",
    "    # Filter out words that are not detected as English\n",
    "    filtered_words = [word for word in words if detect(word) == 'en']\n",
    "    return filtered_words\n",
    "\n",
    "# Update the preprocess function to integrate phrase detection\n",
    "def preprocess_with_phrases(text, phraser):\n",
    "    text = text.lower()\n",
    "    text = \"\".join(re.findall(\"[a-z\\s]*\", text))\n",
    "    words = text.split()\n",
    "    words = remove_non_english_words(words)\n",
    " # Apply phrase model\n",
    "    # filtered_text = [word for word in words if word not in stop_words]\n",
    "    filtered_text = [word for word in words if len(word) > 2 and '_' not in word and word not in stop_words]\n",
    "    filtered_text = phraser[filtered_text] \n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    return lemmatized_text\n",
    "\n",
    "# First, tokenize the documents for phrase detection\n",
    "tokenized_docs = [[word for word in document.lower().split() if word not in stop_words] for document in documents_df['document']]\n",
    "\n",
    "# Detect phrases\n",
    "phraser = detect_phrases(tokenized_docs)\n",
    "\n",
    "# Now preprocess documents including the detected phrases\n",
    "processed_docs_with_phrases = [preprocess_with_phrases(text, phraser) for text in documents_df['document']]\n",
    "\n",
    "# Continue with dictionary and bow_corpus creation using the processed_docs_with_phrases\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs_with_phrases)\n",
    "dictionary.filter_extremes(no_below=6, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs_with_phrases]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tfidf = models.TfidfModel(bow_corpus)\n",
    "# corpus_tfidf = tfidf[bow_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'rank' is the column in documents_df that contains the rankings\n",
    "external_metrics = documents_df['rank'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 6 topics saved at model/lda_model_6.model with coherence score 0.35797974727309206\n",
      "Model with 10 topics saved at model/lda_model_10.model with coherence score 0.4398457492842652\n",
      "Model with 15 topics saved at model/lda_model_15.model with coherence score 0.36244324559489505\n",
      "Model with 20 topics saved at model/lda_model_20.model with coherence score 0.3888987775318332\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "import numpy as np\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "\n",
    "def evaluate_model_coherence(lda_model, texts, dictionary, coherence='c_v'):\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence=coherence)\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def train_and_evaluate_models(corpus, id2word, texts, num_topics_list, passes=10, random_state=42, top_n_models=3, checkpoint_dir=\"model/\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_scores = []\n",
    "    model_paths = []\n",
    "\n",
    "    for num_topics in num_topics_list:\n",
    "        model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics, \n",
    "                             passes=passes, random_state=random_state, workers=4)\n",
    "        model_path = os.path.join(checkpoint_dir, f\"lda_model_{num_topics}.model\")\n",
    "        model.save(model_path)\n",
    "        model_paths.append(model_path)\n",
    "        \n",
    "        coherence_score = evaluate_model_coherence(model, texts, id2word)\n",
    "        model_list.append(model)\n",
    "        coherence_scores.append(coherence_score)\n",
    "        print(f\"Model with {num_topics} topics saved at {model_path} with coherence score {coherence_score}\")\n",
    "\n",
    "    top_indices = np.argsort(coherence_scores)[-top_n_models:]\n",
    "    top_models = [model_list[i] for i in top_indices]\n",
    "    top_scores = [coherence_scores[i] for i in top_indices]\n",
    "    top_model_paths = [model_paths[i] for i in top_indices]\n",
    "\n",
    "    return top_models, top_scores, top_model_paths\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def process_models_and_extract_features(top_models, corpus, id2word, num_topics):\n",
    "    # Determine the maximum number of topics among all models and the specified number of topics\n",
    "    max_num_topics = max(model.num_topics for model in top_models)\n",
    "    num_topics = min(max_num_topics, num_topics)\n",
    "    \n",
    "    feature_vectors = np.zeros((len(corpus), num_topics))\n",
    "    averaged_topics = [[] for _ in range(num_topics)]\n",
    "\n",
    "    # Iterate over each document in the corpus\n",
    "    for i, doc_bow in enumerate(corpus):\n",
    "        doc_topics_avg = np.zeros(num_topics)\n",
    "        \n",
    "        # Iterate over each model to get the topic distribution for the document\n",
    "        for model in top_models:\n",
    "            doc_topics = dict(model.get_document_topics(doc_bow, minimum_probability=0))\n",
    "            for topic_num, prob in doc_topics.items():\n",
    "                if topic_num < num_topics:  # Ensure topic_num is within the specified number of topics\n",
    "                    doc_topics_avg[topic_num] += prob / len(top_models)\n",
    "        \n",
    "        # Update the feature vector for the document\n",
    "        feature_vectors[i, :] = doc_topics_avg\n",
    "    \n",
    "    # Collect and average topic terms across models\n",
    "    for topic_num in range(num_topics):\n",
    "        topic_terms = {}\n",
    "        for model in top_models:\n",
    "            if topic_num < model.num_topics:  # Ensure topic_num is within the model's number of topics\n",
    "                for term_id, weight in model.get_topic_terms(topic_num, topn=20):\n",
    "                    topic_terms[term_id] = topic_terms.get(term_id, 0) + weight / len(top_models)\n",
    "        averaged_terms = sorted(topic_terms.items(), key=lambda x: -x[1])[:20]\n",
    "        averaged_topics[topic_num] = averaged_terms\n",
    "    \n",
    "    # Print averaged topics\n",
    "    print(\"\\nAveraged Topics:\")\n",
    "    for idx, terms in enumerate(averaged_topics):\n",
    "        if terms:  # Only print if there are terms for this topic\n",
    "            terms_str = \" + \".join([f\"{weight:.3f}*{id2word[term]}\" for term, weight in terms])\n",
    "            print(f\"Topic {idx}: {terms_str}\")\n",
    "\n",
    "    return feature_vectors\n",
    "\n",
    "\n",
    "\n",
    "def calculate_correlation(feature_vectors, external_metrics):\n",
    "    correlations = []\n",
    "    for i in range(feature_vectors.shape[1]):\n",
    "        correlation, _ = pearsonr(feature_vectors[:, i], external_metrics)\n",
    "        correlations.append(correlation)\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "\n",
    "num_topics_list = [6, 10, 15, 20]\n",
    "top_n_models = 2\n",
    "\n",
    "top_models, top_scores, top_model_paths = train_and_evaluate_models(bow_corpus, dictionary, texts=processed_docs_with_phrases, num_topics_list=num_topics_list, top_n_models=top_n_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averaged Topics:\n",
      "Topic 0: 0.021*msc + 0.020*engineering + 0.009*centre + 0.008*bsc + 0.008*del + 0.007*programme + 0.005*master + 0.005*aau + 0.005*pbl + 0.005*industrial + 0.004*energy + 0.004*agricultural + 0.004*architecture + 0.004*ear + 0.003*food + 0.003*option + 0.003*problem + 0.003*organisation + 0.003*mining + 0.003*feature\n",
      "Topic 1: 0.007*strengthening + 0.005*center + 0.005*organization + 0.004*construction + 0.004*evaluation + 0.003*fall + 0.003*promoting + 0.003*joint + 0.003*table + 0.003*china + 0.002*scientific + 0.002*sdgs + 0.002*cooperation + 0.002*unit + 0.002*production + 0.002*total + 0.002*target + 0.002*improving + 0.002*term + 0.002*japan\n",
      "Topic 2: 0.027*york + 0.012*indigenous + 0.012*queen + 0.008*scholarship + 0.007*centre + 0.007*scholar + 0.006*canada + 0.005*environmental + 0.005*equity + 0.005*fundamental + 0.005*engineering + 0.004*canadian + 0.004*material + 0.004*discovery + 0.004*art + 0.004*disease + 0.003*governance + 0.003*law + 0.003*creation + 0.001*application\n",
      "Topic 3: 0.006*deliver + 0.005*ambition + 0.004*programme + 0.004*enterprise + 0.003*outcome + 0.003*city + 0.003*theme + 0.003*supporting + 0.003*establishment + 0.003*beyond + 0.002*estate + 0.002*creative + 0.002*accountability + 0.002*centre + 0.002*startup + 0.002*grow + 0.002*outstanding + 0.002*wellbeing + 0.002*target + 0.002*creativity\n",
      "Topic 4: 0.007*programme + 0.003*centre + 0.003*organisation + 0.002*cooperation + 0.002*deliver + 0.002*scientific + 0.002*wellbeing + 0.002*context + 0.002*grow + 0.002*internationalisation + 0.002*term + 0.002*outcome + 0.001*creation + 0.001*european + 0.001*invest + 0.001*particular + 0.001*reputation + 0.001*addition + 0.001*therefore + 0.001*internal\n",
      "Topic 5: 0.026*que + 0.015*del + 0.011*mit + 0.007*con + 0.007*belonging + 0.006*una + 0.006*por + 0.006*composition + 0.005*sus + 0.004*provided + 0.004*cooperation + 0.004*evaluation + 0.004*foreign + 0.004*postdoc + 0.004*internal + 0.003*mobility + 0.003*provost + 0.003*programme + 0.003*employee + 0.003*abroad\n",
      "Topic 6: 0.015*universit + 0.006*paris + 0.005*scientific + 0.005*bilingual + 0.004*programme + 0.004*wellbeing + 0.004*language + 0.003*promoting + 0.003*sta + 0.003*provision + 0.003*underpinned + 0.003*aligned + 0.003*supporting + 0.003*longterm + 0.003*delivery + 0.003*environmental + 0.002*civic + 0.002*excellent + 0.002*outcome + 0.002*scale\n",
      "Topic 7: 0.007*center + 0.005*scientific + 0.003*technological + 0.003*different + 0.003*production + 0.003*engineering + 0.003*iit + 0.003*scientist + 0.003*dissemination + 0.002*general + 0.002*specific + 0.002*teacher + 0.002*extension + 0.002*promotion + 0.002*material + 0.002*energy + 0.002*evaluation + 0.002*healthcare + 0.002*clinical + 0.002*european\n",
      "Topic 8: 0.015*college + 0.008*center + 0.007*state + 0.006*office + 0.005*equity + 0.005*president + 0.005*scholarship + 0.004*committee + 0.004*inclusion + 0.004*metric + 0.003*discovery + 0.003*dean + 0.003*review + 0.002*director + 0.002*fall + 0.002*climate + 0.002*water + 0.002*professor + 0.002*beyond + 0.002*retention\n",
      "Topic 9: 0.006*deliver + 0.006*aspiration + 0.004*organisation + 0.004*outcome + 0.004*capability + 0.004*sydney + 0.004*supporting + 0.004*drive + 0.003*globally + 0.003*ambition + 0.003*transformational + 0.003*programme + 0.003*improvement + 0.003*scholarship + 0.003*wellbeing + 0.002*realise + 0.002*australia + 0.002*aboriginal + 0.002*kpi + 0.002*american\n"
     ]
    }
   ],
   "source": [
    "chosen_number_of_topics=10\n",
    "feature_vectors = process_models_and_extract_features(top_models, corpus=bow_corpus, id2word=dictionary, num_topics=chosen_number_of_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Correlation = 0.0694321376869563\n",
      "Topic 1: Correlation = 0.10311634020106784\n",
      "Topic 2: Correlation = -0.013423427241307568\n",
      "Topic 3: Correlation = -0.13530900346238692\n",
      "Topic 4: Correlation = -0.11804120977449338\n",
      "Topic 5: Correlation = 0.11027110446022696\n",
      "Topic 6: Correlation = 0.11324648981019557\n",
      "Topic 7: Correlation = -0.02082309242622215\n",
      "Topic 8: Correlation = 0.07808711264243345\n",
      "Topic 9: Correlation = -0.03965137982762084\n"
     ]
    }
   ],
   "source": [
    "def calculate_correlation(feature_vectors, external_metrics):\n",
    "    correlations = []\n",
    "    for i in range(feature_vectors.shape[1]):\n",
    "        correlation, _ = pearsonr(feature_vectors[:, i], external_metrics)\n",
    "        correlations.append(correlation)\n",
    "    return correlations\n",
    "\n",
    "# Perform correlation analysis\n",
    "correlations = calculate_correlation(feature_vectors, external_metrics)\n",
    "for idx, corr in enumerate(correlations):\n",
    "    print(f\"Topic {idx}: Correlation = {corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>rank</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>University of Wisconsin-Madison</td>\n",
       "      <td>102</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.072534</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.067232</td>\n",
       "      <td>0.258236</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.584896</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Pontifical Catholic University of Chile</td>\n",
       "      <td>103</td>\n",
       "      <td>0.071030</td>\n",
       "      <td>0.211017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.206379</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.073620</td>\n",
       "      <td>0.038851</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The University of Sheffield</td>\n",
       "      <td>104</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.956485</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.042269</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uppsala University</td>\n",
       "      <td>105</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.657056</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>University of Copenhagen</td>\n",
       "      <td>107</td>\n",
       "      <td>0.012755</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.158821</td>\n",
       "      <td>0.375748</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.159480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Belarusian State University</td>\n",
       "      <td>387</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.739682</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.065905</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Institut National des Sciences AppliquÃ©es de ...</td>\n",
       "      <td>392</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.292336</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.072312</td>\n",
       "      <td>0.028619</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.105060</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>South China University of Technology</td>\n",
       "      <td>392</td>\n",
       "      <td>0.062404</td>\n",
       "      <td>0.369316</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.065985</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.496458</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Justus-Liebig-University Giessen</td>\n",
       "      <td>396</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.007645</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.491169</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>HSE University</td>\n",
       "      <td>399</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.499820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name  rank  feature_1  \\\n",
       "0                     University of Wisconsin-Madison    102   0.000164   \n",
       "1          The Pontifical Catholic University of Chile   103   0.071030   \n",
       "2                          The University of Sheffield   104   0.000110   \n",
       "3                                   Uppsala University   105   0.000064   \n",
       "4                             University of Copenhagen   107   0.012755   \n",
       "..                                                 ...   ...        ...   \n",
       "248                        Belarusian State University   387   0.000352   \n",
       "249  Institut National des Sciences AppliquÃ©es de ...   392   0.000100   \n",
       "250               South China University of Technology   392   0.062404   \n",
       "251                   Justus-Liebig-University Giessen   396   0.000018   \n",
       "252                                     HSE University   399   0.000032   \n",
       "\n",
       "     feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
       "0     0.072534   0.000164   0.067232   0.258236   0.000164   0.000164   \n",
       "1     0.211017   0.000016   0.000016   0.206379   0.000016   0.000016   \n",
       "2     0.000110   0.000110   0.956485   0.000110   0.000110   0.000110   \n",
       "3     0.000064   0.000064   0.000064   0.657056   0.000064   0.000064   \n",
       "4     0.000221   0.000221   0.158821   0.375748   0.000221   0.000221   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "248   0.739682   0.000352   0.000352   0.000352   0.065905   0.000352   \n",
       "249   0.292336   0.000100   0.000100   0.072312   0.028619   0.001507   \n",
       "250   0.369316   0.000625   0.065985   0.000625   0.000625   0.496458   \n",
       "251   0.007645   0.000018   0.000018   0.491169   0.000395   0.000018   \n",
       "252   0.000032   0.000032   0.000032   0.000032   0.000032   0.000032   \n",
       "\n",
       "     feature_8  feature_9  feature_10  \n",
       "0     0.000164   0.584896    0.000164  \n",
       "1     0.073620   0.038851    0.000016  \n",
       "2     0.000110   0.042269    0.000110  \n",
       "3     0.002237   0.000064    0.000064  \n",
       "4     0.000221   0.000221    0.159480  \n",
       "..         ...        ...         ...  \n",
       "248   0.000352   0.000352    0.000352  \n",
       "249   0.000100   0.105060    0.000100  \n",
       "250   0.000625   0.000625    0.000625  \n",
       "251   0.000745   0.000018    0.000018  \n",
       "252   0.000032   0.000032    0.499820  \n",
       "\n",
       "[253 rows x 12 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame({\n",
    "    'Name': documents_df['institution'],\n",
    "    'rank': documents_df['rank']\n",
    "})\n",
    "num_features = len(feature_vectors[0])  # Get the number of features\n",
    "for i in range(num_features):\n",
    "    final_df[f'feature_{i+1}'] = [vector[i] for vector in feature_vectors]\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/final_file_bow.csv'\n",
    "\n",
    "# Dump the DataFrame to a CSV file\n",
    "final_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
