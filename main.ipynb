{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you've already loaded your DataFrame `alignment_df` from the CSV\n",
    "directory_path = '/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/data/raw-txt'\n",
    "\n",
    "alignment_df = pd.read_csv('/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/data/combined_csv.csv')\n",
    "def ensure_txt_extension(filename):\n",
    "    if pd.isnull(filename):\n",
    "        return None  # Handles NaN values gracefully\n",
    "    if not filename.endswith('.txt'):\n",
    "        return f\"{filename}.txt\"\n",
    "    return filename\n",
    "alignment_df['filename'] = alignment_df['filename'].apply(ensure_txt_extension)\n",
    "\n",
    "# Function to read text from a .txt file\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "documents_list = []\n",
    "ranks = []\n",
    "uni_names = []\n",
    "\n",
    "for index, row in alignment_df.iterrows():\n",
    "    # Check if the filename is not NaN (not a float)\n",
    "    if not pd.isna(row['filename']) and isinstance(row['filename'], str):\n",
    "        file_path = os.path.join(directory_path, row['filename'])\n",
    "        if os.path.exists(file_path):\n",
    "            text = read_txt(file_path)\n",
    "            documents_list.append(text)\n",
    "            ranks.append(row['qs_ranking'])\n",
    "            uni_names.append(row['institution'])\n",
    "        else:\n",
    "            print(f\"File does not exist: {file_path}\")\n",
    "    else:\n",
    "        # Handle the case where filename is NaN or not a string\n",
    "        print(f\"Invalid or missing filename at index {index}\")\n",
    "\n",
    "# Convert lists to DataFrame\n",
    "documents_df = pd.DataFrame({\n",
    "    'document': documents_list,\n",
    "    'rank': ranks,\n",
    "    'institution': uni_names\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>rank</th>\n",
       "      <th>institution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FINAL : APPROVED DECEMBER 8 , 2022 \\n \\n20...</td>\n",
       "      <td>102</td>\n",
       "      <td>University of Wisconsin-Madison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLAN\\n2020 • 2025DEVELOPMENT\\nPLAN\\n2020 • 202...</td>\n",
       "      <td>103</td>\n",
       "      <td>The Pontifical Catholic University of Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GO FURTHER2021–2025\\nTransforming lives  \\nthr...</td>\n",
       "      <td>104</td>\n",
       "      <td>The University of Sheffield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n UFV 2018/641  \\nUppsala University: \\nMis...</td>\n",
       "      <td>105</td>\n",
       "      <td>Uppsala University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>university of copenhagen\\nSTRATEGY 2030\\nCreat...</td>\n",
       "      <td>107</td>\n",
       "      <td>University of Copenhagen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>BSU BSU ––an an \\nexperienced experienced \\nan...</td>\n",
       "      <td>387</td>\n",
       "      <td>Belarusian State University</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>July 2023 - Case Number : 2019FR470010  \\n  \\n...</td>\n",
       "      <td>392</td>\n",
       "      <td>Institut National des Sciences AppliquÃ©es de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Prospective Student (http://sites.scut.edu.cn/...</td>\n",
       "      <td>392</td>\n",
       "      <td>South China University of Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>THE DEVELOPMENT PLAN OF JUSTUS LIEBIG UNIVERSI...</td>\n",
       "      <td>396</td>\n",
       "      <td>Justus-Liebig-University Giessen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>\\n                                          \\...</td>\n",
       "      <td>399</td>\n",
       "      <td>HSE University</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              document  rank  \\\n",
       "0        FINAL : APPROVED DECEMBER 8 , 2022 \\n \\n20...   102   \n",
       "1    PLAN\\n2020 • 2025DEVELOPMENT\\nPLAN\\n2020 • 202...   103   \n",
       "2    GO FURTHER2021–2025\\nTransforming lives  \\nthr...   104   \n",
       "3      \\n UFV 2018/641  \\nUppsala University: \\nMis...   105   \n",
       "4    university of copenhagen\\nSTRATEGY 2030\\nCreat...   107   \n",
       "..                                                 ...   ...   \n",
       "248  BSU BSU ––an an \\nexperienced experienced \\nan...   387   \n",
       "249  July 2023 - Case Number : 2019FR470010  \\n  \\n...   392   \n",
       "250  Prospective Student (http://sites.scut.edu.cn/...   392   \n",
       "251  THE DEVELOPMENT PLAN OF JUSTUS LIEBIG UNIVERSI...   396   \n",
       "252   \\n                                          \\...   399   \n",
       "\n",
       "                                           institution  \n",
       "0                     University of Wisconsin-Madison   \n",
       "1          The Pontifical Catholic University of Chile  \n",
       "2                          The University of Sheffield  \n",
       "3                                   Uppsala University  \n",
       "4                             University of Copenhagen  \n",
       "..                                                 ...  \n",
       "248                        Belarusian State University  \n",
       "249  Institut National des Sciences AppliquÃ©es de ...  \n",
       "250               South China University of Technology  \n",
       "251                   Justus-Liebig-University Giessen  \n",
       "252                                     HSE University  \n",
       "\n",
       "[253 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# import re\n",
    "# import gensim\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# def preprocess(text):\n",
    "#     text = text.lower()\n",
    "#     text = \"\".join(re.findall(\"[a-z\\s]*\", text)) \n",
    "#     words = text.split() \n",
    "#     filtered_text = [word for word in words if word not in stop_words]\n",
    "#     lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "#     return lemmatized_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nachiketh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nachiketh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nachiketh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/nachiketh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Function to detect phrases in the documents\n",
    "def detect_phrases(docs):\n",
    "    phrases = Phrases(docs, min_count=5, threshold=12) # Play with these parameters based on your corpus\n",
    "    phraser = Phraser(phrases)\n",
    "    return phraser\n",
    "\n",
    "# Update the preprocess function to integrate phrase detection\n",
    "def preprocess_with_phrases(text, phraser):\n",
    "    text = text.lower()\n",
    "    text = \"\".join(re.findall(\"[a-z\\s]*\", text))\n",
    "    words = text.split()\n",
    "    words = phraser[words]  # Apply phrase model\n",
    "    # filtered_text = [word for word in words if word not in stop_words]\n",
    "    filtered_text = [word for word in words if len(word) > 2 and '_' not in word and word not in stop_words]\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_text]\n",
    "    return lemmatized_text\n",
    "\n",
    "# First, tokenize the documents for phrase detection\n",
    "tokenized_docs = [[word for word in document.lower().split() if word not in stop_words] for document in documents_df['document']]\n",
    "\n",
    "# Detect phrases\n",
    "phraser = detect_phrases(tokenized_docs)\n",
    "\n",
    "# Now preprocess documents including the detected phrases\n",
    "processed_docs_with_phrases = [preprocess_with_phrases(text, phraser) for text in documents_df['document']]\n",
    "\n",
    "# Continue with dictionary and bow_corpus creation using the processed_docs_with_phrases\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs_with_phrases)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs_with_phrases]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming processed_docs is a list of preprocessed documents aligned with external_metrics\n",
    "# processed_docs = [preprocess(text) for text in documents_df['document']]\n",
    "\n",
    "# # Update dictionary and bow_corpus creation steps accordingly\n",
    "# dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "# dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# from gensim import corpora, models\n",
    "\n",
    "# tfidf = models.TfidfModel(bow_corpus)\n",
    "# corpus_tfidf = tfidf[bow_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'rank' is the column in documents_df that contains the rankings\n",
    "external_metrics = documents_df['rank'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 6 topics saved at model/lda_model_6.model with coherence score 0.35446738165382485\n",
      "Model with 10 topics saved at model/lda_model_10.model with coherence score 0.383880088600114\n",
      "Model with 15 topics saved at model/lda_model_15.model with coherence score 0.3892610254017365\n",
      "Model with 20 topics saved at model/lda_model_20.model with coherence score 0.3646888565521921\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "import numpy as np\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "\n",
    "def evaluate_model_coherence(lda_model, texts, dictionary, coherence='c_v'):\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence=coherence)\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def train_and_evaluate_models(corpus, id2word, texts, num_topics_list, passes=10, random_state=42, top_n_models=3, checkpoint_dir=\"model/\"):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_scores = []\n",
    "    model_paths = []\n",
    "\n",
    "    for num_topics in num_topics_list:\n",
    "        model = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics, \n",
    "                             passes=passes, random_state=random_state, workers=4)\n",
    "        model_path = os.path.join(checkpoint_dir, f\"lda_model_{num_topics}.model\")\n",
    "        model.save(model_path)\n",
    "        model_paths.append(model_path)\n",
    "        \n",
    "        coherence_score = evaluate_model_coherence(model, texts, id2word)\n",
    "        model_list.append(model)\n",
    "        coherence_scores.append(coherence_score)\n",
    "        print(f\"Model with {num_topics} topics saved at {model_path} with coherence score {coherence_score}\")\n",
    "\n",
    "    top_indices = np.argsort(coherence_scores)[-top_n_models:]\n",
    "    top_models = [model_list[i] for i in top_indices]\n",
    "    top_scores = [coherence_scores[i] for i in top_indices]\n",
    "    top_model_paths = [model_paths[i] for i in top_indices]\n",
    "\n",
    "    return top_models, top_scores, top_model_paths\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def process_models_and_extract_features(top_models, corpus, id2word, num_topics):\n",
    "    # Determine the maximum number of topics among all models and the specified number of topics\n",
    "    max_num_topics = max(model.num_topics for model in top_models)\n",
    "    num_topics = min(max_num_topics, num_topics)\n",
    "    \n",
    "    feature_vectors = np.zeros((len(corpus), num_topics))\n",
    "    averaged_topics = [[] for _ in range(num_topics)]\n",
    "\n",
    "    # Iterate over each document in the corpus\n",
    "    for i, doc_bow in enumerate(corpus):\n",
    "        doc_topics_avg = np.zeros(num_topics)\n",
    "        \n",
    "        # Iterate over each model to get the topic distribution for the document\n",
    "        for model in top_models:\n",
    "            doc_topics = dict(model.get_document_topics(doc_bow, minimum_probability=0))\n",
    "            for topic_num, prob in doc_topics.items():\n",
    "                if topic_num < num_topics:  # Ensure topic_num is within the specified number of topics\n",
    "                    doc_topics_avg[topic_num] += prob / len(top_models)\n",
    "        \n",
    "        # Update the feature vector for the document\n",
    "        feature_vectors[i, :] = doc_topics_avg\n",
    "    \n",
    "    # Collect and average topic terms across models\n",
    "    for topic_num in range(num_topics):\n",
    "        topic_terms = {}\n",
    "        for model in top_models:\n",
    "            if topic_num < model.num_topics:  # Ensure topic_num is within the model's number of topics\n",
    "                for term_id, weight in model.get_topic_terms(topic_num, topn=20):\n",
    "                    topic_terms[term_id] = topic_terms.get(term_id, 0) + weight / len(top_models)\n",
    "        averaged_terms = sorted(topic_terms.items(), key=lambda x: -x[1])[:20]\n",
    "        averaged_topics[topic_num] = averaged_terms\n",
    "    \n",
    "    # Print averaged topics\n",
    "    print(\"\\nAveraged Topics:\")\n",
    "    for idx, terms in enumerate(averaged_topics):\n",
    "        if terms:  # Only print if there are terms for this topic\n",
    "            terms_str = \" + \".join([f\"{weight:.3f}*{id2word[term]}\" for term, weight in terms])\n",
    "            print(f\"Topic {idx}: {terms_str}\")\n",
    "\n",
    "    return feature_vectors\n",
    "\n",
    "\n",
    "\n",
    "def calculate_correlation(feature_vectors, external_metrics):\n",
    "    correlations = []\n",
    "    for i in range(feature_vectors.shape[1]):\n",
    "        correlation, _ = pearsonr(feature_vectors[:, i], external_metrics)\n",
    "        correlations.append(correlation)\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "\n",
    "num_topics_list = [6, 10, 15, 20]\n",
    "top_n_models = 2\n",
    "\n",
    "top_models, top_scores, top_model_paths = train_and_evaluate_models(bow_corpus, dictionary, texts=processed_docs_with_phrases, num_topics_list=num_topics_list, top_n_models=top_n_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Averaged Topics:\n",
      "Topic 0: 0.006*programme + 0.004*scientific + 0.004*creation + 0.003*professor + 0.003*organisation + 0.003*term + 0.003*strengthening + 0.003*promoting + 0.003*context + 0.003*addition + 0.003*therefore + 0.002*cooperation + 0.002*different + 0.002*office + 0.002*centre + 0.002*european + 0.002*pillar + 0.001*kpi + 0.001*employee + 0.001*internal\n",
      "Topic 1: 0.012*universit + 0.010*iit + 0.010*center + 0.009*scientific + 0.007*iits + 0.007*paris + 0.005*european + 0.004*laboratory + 0.004*hospital + 0.003*table + 0.003*environmental + 0.003*industrial + 0.002*college + 0.002*unit + 0.002*robotics + 0.002*water + 0.002*company + 0.002*total + 0.002*startup + 0.002*programme\n",
      "Topic 2: 0.005*establishment + 0.005*china + 0.004*semester + 0.004*msc + 0.004*humanity + 0.004*cooperation + 0.004*strengthening + 0.003*engineering + 0.003*beyond + 0.003*korea + 0.003*center + 0.003*centre + 0.003*programme + 0.003*construction + 0.003*startup + 0.002*creative + 0.002*overseas + 0.002*party + 0.002*platform + 0.002*promoting\n",
      "Topic 3: 0.010*york + 0.008*equity + 0.006*indigenous + 0.006*scholarship + 0.006*fall + 0.006*office + 0.005*college + 0.005*inclusion + 0.004*president + 0.004*queen + 0.004*scholar + 0.004*discovery + 0.003*population + 0.003*state + 0.003*committee + 0.002*professor + 0.002*provost + 0.002*director + 0.002*belonging + 0.002*wellbeing\n",
      "Topic 4: 0.011*epfl + 0.010*center + 0.008*engineering + 0.006*scientific + 0.005*energy + 0.005*material + 0.004*scientist + 0.004*platform + 0.003*master + 0.003*engineer + 0.003*method + 0.003*joint + 0.003*universit + 0.003*technological + 0.003*application + 0.003*fundamental + 0.002*promoting + 0.001*switzerland + 0.001*professor + 0.001*environmental\n",
      "Topic 5: 0.010*sdgs + 0.007*scholarship + 0.006*accountability + 0.006*american + 0.006*college + 0.005*center + 0.004*sdg + 0.004*climate + 0.003*committee + 0.003*ambition + 0.003*organization + 0.002*want + 0.002*four + 0.002*dei + 0.002*engineering + 0.002*comprehensive + 0.002*arab + 0.002*scientific + 0.002*professor + 0.002*water\n",
      "Topic 6: 0.006*iit + 0.004*internationalization + 0.004*term + 0.004*virtual + 0.003*professor + 0.003*different + 0.003*power + 0.003*mobility + 0.002*competency + 0.002*office + 0.002*endowment + 0.002*scientic + 0.002*promotes + 0.002*alliance + 0.002*wellbeing + 0.002*board + 0.002*cutting + 0.002*cross + 0.002*str + 0.002*kpis\n",
      "Topic 7: 0.006*production + 0.005*evaluation + 0.005*extension + 0.005*teacher + 0.004*organization + 0.004*center + 0.004*general + 0.004*construction + 0.004*different + 0.004*promotion + 0.003*comprehensive + 0.003*cooperation + 0.003*estate + 0.002*foreign + 0.002*internal + 0.002*scientific + 0.002*strengthening + 0.002*provided + 0.002*articulation + 0.002*programme\n",
      "Topic 8: 0.015*programme + 0.008*que + 0.007*improvement + 0.007*ireland + 0.007*deliver + 0.007*organisation + 0.005*ambition + 0.005*wellbeing + 0.005*clinical + 0.003*healthcare + 0.003*del + 0.003*care + 0.003*technological + 0.003*mori + 0.003*patient + 0.002*la + 0.002*outcome + 0.002*los + 0.002*connecting + 0.002*evidence\n",
      "Topic 9: 0.006*college + 0.003*city + 0.003*beyond + 0.002*humanity + 0.002*target + 0.002*estate + 0.002*centre + 0.002*center + 0.002*creative + 0.002*online + 0.001*scholarship + 0.001*ear + 0.001*programme + 0.001*may + 0.001*establishment + 0.001*enterprise + 0.001*entrepreneurship + 0.001*would + 0.001*invest + 0.001*creativity\n"
     ]
    }
   ],
   "source": [
    "chosen_number_of_topics=10\n",
    "feature_vectors = process_models_and_extract_features(top_models, corpus=bow_corpus, id2word=dictionary, num_topics=chosen_number_of_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: Correlation = -0.00113087570087576\n",
      "Topic 1: Correlation = 0.05828495701050555\n",
      "Topic 2: Correlation = 0.04057677178520706\n",
      "Topic 3: Correlation = 0.0134694096797457\n",
      "Topic 4: Correlation = -0.16785769975179315\n",
      "Topic 5: Correlation = -0.031010928688843007\n",
      "Topic 6: Correlation = -0.043958803283330954\n",
      "Topic 7: Correlation = 0.15098224999109386\n",
      "Topic 8: Correlation = 0.06783083357119717\n",
      "Topic 9: Correlation = -0.1721703477981967\n"
     ]
    }
   ],
   "source": [
    "def calculate_correlation(feature_vectors, external_metrics):\n",
    "    correlations = []\n",
    "    for i in range(feature_vectors.shape[1]):\n",
    "        correlation, _ = pearsonr(feature_vectors[:, i], external_metrics)\n",
    "        correlations.append(correlation)\n",
    "    return correlations\n",
    "\n",
    "# Perform correlation analysis\n",
    "correlations = calculate_correlation(feature_vectors, external_metrics)\n",
    "for idx, corr in enumerate(correlations):\n",
    "    print(f\"Topic {idx}: Correlation = {corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>rank</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>University of Wisconsin-Madison</td>\n",
       "      <td>102</td>\n",
       "      <td>0.102442</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.444332</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.114312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Pontifical Catholic University of Chile</td>\n",
       "      <td>103</td>\n",
       "      <td>0.591698</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.329850</td>\n",
       "      <td>0.077522</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The University of Sheffield</td>\n",
       "      <td>104</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.013050</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.656555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uppsala University</td>\n",
       "      <td>105</td>\n",
       "      <td>0.504582</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.011733</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>University of Copenhagen</td>\n",
       "      <td>107</td>\n",
       "      <td>0.047874</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.254257</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.498789</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.178663</td>\n",
       "      <td>0.018720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Belarusian State University</td>\n",
       "      <td>387</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.023160</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.304012</td>\n",
       "      <td>0.194165</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Institut National des Sciences AppliquÃ©es de ...</td>\n",
       "      <td>392</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.837890</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.012787</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>South China University of Technology</td>\n",
       "      <td>392</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.185460</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.122238</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.169379</td>\n",
       "      <td>0.005998</td>\n",
       "      <td>0.273779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Justus-Liebig-University Giessen</td>\n",
       "      <td>396</td>\n",
       "      <td>0.481001</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>HSE University</td>\n",
       "      <td>399</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.036714</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.961683</td>\n",
       "      <td>0.000034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name  rank  feature_1  \\\n",
       "0                     University of Wisconsin-Madison    102   0.102442   \n",
       "1          The Pontifical Catholic University of Chile   103   0.591698   \n",
       "2                          The University of Sheffield   104   0.000121   \n",
       "3                                   Uppsala University   105   0.504582   \n",
       "4                             University of Copenhagen   107   0.047874   \n",
       "..                                                 ...   ...        ...   \n",
       "248                        Belarusian State University   387   0.000391   \n",
       "249  Institut National des Sciences AppliquÃ©es de ...   392   0.000107   \n",
       "250               South China University of Technology   392   0.000684   \n",
       "251                   Justus-Liebig-University Giessen   396   0.481001   \n",
       "252                                     HSE University   399   0.000034   \n",
       "\n",
       "     feature_2  feature_3  feature_4  feature_5  feature_6  feature_7  \\\n",
       "0     0.000180   0.000180   0.444332   0.000180   0.000180   0.000180   \n",
       "1     0.000017   0.000017   0.000790   0.000017   0.000017   0.329850   \n",
       "2     0.000121   0.000121   0.000121   0.000121   0.000121   0.000121   \n",
       "3     0.000070   0.000070   0.000070   0.000070   0.000070   0.000070   \n",
       "4     0.000242   0.254257   0.000242   0.000242   0.498789   0.000242   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "248   0.000392   0.000391   0.000391   0.023160   0.000391   0.304012   \n",
       "249   0.000107   0.837890   0.000107   0.000107   0.000107   0.000107   \n",
       "250   0.000684   0.185460   0.000684   0.122238   0.000684   0.000684   \n",
       "251   0.000030   0.000019   0.000019   0.018059   0.000019   0.000019   \n",
       "252   0.036714   0.000034   0.001295   0.000034   0.000034   0.000034   \n",
       "\n",
       "     feature_8  feature_9  feature_10  \n",
       "0     0.000180   0.000180    0.114312  \n",
       "1     0.077522   0.000017    0.000017  \n",
       "2     0.013050   0.009960    0.656555  \n",
       "3     0.000070   0.011733    0.000070  \n",
       "4     0.000242   0.178663    0.018720  \n",
       "..         ...        ...         ...  \n",
       "248   0.194165   0.000391    0.000391  \n",
       "249   0.012787   0.000107    0.000107  \n",
       "250   0.169379   0.005998    0.273779  \n",
       "251   0.000871   0.000019    0.000019  \n",
       "252   0.000034   0.961683    0.000034  \n",
       "\n",
       "[253 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame({\n",
    "    'Name': documents_df['institution'],\n",
    "    'rank': documents_df['rank']\n",
    "})\n",
    "num_features = len(feature_vectors[0])  # Get the number of features\n",
    "for i in range(num_features):\n",
    "    final_df[f'feature_{i+1}'] = [vector[i] for vector in feature_vectors]\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/Users/nachiketh/Library/CloudStorage/OneDrive-TrinityCollegeDublin/Text_analytics/project/final_file_bow.csv'\n",
    "\n",
    "# Dump the DataFrame to a CSV file\n",
    "final_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
